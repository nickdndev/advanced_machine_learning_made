{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Продвинутое машинное обучение: \n",
    "### Домашнее задание 3\n",
    "\n",
    "Третье домашнее задание посвящено достаточно простой, но, надеюсь, интересной задаче, в которой потребуется творчески применить методы сэмплирования. Как и раньше, в качестве решения ожидается ссылка на jupyter-ноутбук на вашем github (или публичный, или с доступом для snikolenko); ссылку обязательно нужно прислать в виде сданного домашнего задания на портале Академии. Как всегда, любые комментарии, новые идеи и рассуждения на тему категорически приветствуются. \n",
    "В этом небольшом домашнем задании мы попробуем улучшить метод Шерлока Холмса.\n",
    "\n",
    "Пользовался он для этого так называемым частотным методом: смотрел, какие буквы чаще встречаются в зашифрованных текстах, и пытался подставить буквы в соответствии с частотной таблицей: E — самая частая и так далее.\n",
    "В этом задании мы будем разрабатывать более современный и продвинутый вариант такого частотного метода. В качестве корпусов текстов для подсчётов частот можете взять что угодно, но для удобства вот вам “Война и мир” по-русски и по-английски:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Реализуйте базовый частотный метод по Шерлоку Холмсу:\n",
    "- подсчитайте частоты букв по корпусам (пунктуацию и капитализацию можно просто опустить, а вот пробелы лучше оставить);\n",
    "- возьмите какие-нибудь тестовые тексты (нужно взять по меньшей мере 2-3 предложения, иначе вряд ли сработает), зашифруйте их посредством случайной перестановки символов;\n",
    "- расшифруйте их таким частотным методом.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import random\n",
    "from copy import copy\n",
    "\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import everygrams\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from multiprocessing import Pool\n",
    "from functools import reduce\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENS = \"абвгдежзийклмнопрстуфхцчшщъыьэюя \"\n",
    "TOKENS_LIST = list(TOKENS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**В качестве основного корпуса будем использовать произведение \"Война и Мир\" , а для расшифровки будем использовать произведение \"Анна Каренина\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_anna_kerenina = \"./data/AnnaKarenina.txt\"\n",
    "data_path_war_and_piece = \"./data/WarAndPeace.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path_anna_kerenina, \"r\") as f:\n",
    "    anna_kerenina_text = f.read().lower()\n",
    "    anna_kerenina_text_filtered = \"\".join(\n",
    "        [token for token in anna_kerenina_text if token.lower() in TOKENS_LIST]\n",
    "    )\n",
    "    anna_kerenina_counter = Counter(anna_kerenina_text_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path_war_and_piece, \"r\") as f:\n",
    "    war_and_piece_text = f.read().lower()\n",
    "    war_and_piece_text_filtered = \"\".join(\n",
    "        [token for token in war_and_piece_text if token.lower() in TOKENS_LIST]\n",
    "    )\n",
    "    war_and_piece_counter = Counter(war_and_piece_text_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_letters(counter):\n",
    "    number = 0\n",
    "    for i in range(len(TOKENS_LIST)):\n",
    "        number += counter[TOKENS_LIST[i]]\n",
    "    return number\n",
    "\n",
    "\n",
    "def frequency(number, counter):\n",
    "    arr_frequency = []\n",
    "    for i in range(len(TOKENS_LIST)):\n",
    "        ratio = counter[TOKENS_LIST[i]] / number * 100\n",
    "        arr_frequency.append(ratio)\n",
    "    return arr_frequency\n",
    "\n",
    "\n",
    "def accuracy(text, decoded_text):\n",
    "    assert len(text) == len(decoded_text)\n",
    "    true_token = 0\n",
    "    for t1, t2 in zip(text, decoded_text):\n",
    "        true_token += int(t1 == t2)\n",
    "    return true_token / len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7ffa446ed430>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_anna_kerenina = count_letters(anna_kerenina_counter)\n",
    "count_war_and_piece = count_letters(war_and_piece_counter)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "xs = range(len(TOKENS_LIST))\n",
    "\n",
    "ax.bar(\n",
    "    [x + 0.05 for x in xs],\n",
    "    frequency(count_war_and_piece, war_and_piece_counter),\n",
    "    width=0.1,\n",
    "    color=\"red\",\n",
    "    alpha=0.7,\n",
    "    label=\"War and Peace\",\n",
    "    zorder=2,\n",
    ")\n",
    "\n",
    "ax.bar(\n",
    "    [x + 0.15 for x in xs],\n",
    "    frequency(count_anna_kerenina, anna_kerenina_counter),\n",
    "    width=0.1,\n",
    "    color=\"blue\",\n",
    "    alpha=0.7,\n",
    "    label=\"Anna Karenina\",\n",
    "    zorder=2,\n",
    ")\n",
    "\n",
    "plt.xticks(xs, TOKENS_LIST)\n",
    "ax.set_facecolor(\"seashell\")\n",
    "fig.set_figwidth(15)\n",
    "fig.set_figheight(10)\n",
    "fig.set_facecolor(\"floralwhite\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Мы видим, что частота появления токенов в двух произведениях примерно одинаковая и использование одного произведения для нахождения частотности токенов уместно**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Зашифруем отрывок из произведения Анна Каренина**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cipher(text):\n",
    "    original_tokens = TOKENS_LIST\n",
    "    encoded_tokens = np.random.choice(\n",
    "        original_tokens, replace=False, size=len(original_tokens)\n",
    "    )\n",
    "    encoder = dict()\n",
    "\n",
    "    for original_token, encoded_token in zip(original_tokens, encoded_tokens):\n",
    "        encoder[original_token] = encoded_token\n",
    "\n",
    "    encoded_text = \"\".join(\n",
    "        [encoder[token] if token in encoder else token for token in text]\n",
    "    )\n",
    "    return encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'анна каренина один из самых знаменитых романов льва толстого начинается ставшей афоризмом фразой все счастливые семьи похожи друг на друга каждая несчастливая семья несчастлива посвоему это книга о вечных ценностях о любви о вере о семье о человеческом достоинстве  лев толстойроман широкого дыханиячасть перваялев толстойанна каренинароман широкого дыханияанна каренина поразила современников вседневностью содержания необычайная свобода раскованность повествования удивительно сочетались в этом романе с цельностью художественного взгляда автора на жизнь он выступал здесь как художник и мыслитель и назначение искусства видел не в том чтобы неоспоримо разрешить вопрос а в том чтобы заставить любить жизнь в бесчисленных никогда не истощимых всех ее проявлениях  в е годы один маститый писатель повидимому гончаров сказал достоевскому это вещь неслыханная это вещь первая кто у нас из писателей может поравняться с этим а в европе  кто представит хоть чтонибудь подобное фм достоевский находил в новом романе толстого огромную психологическую разработку души человеческой страшную глубину и силу и главное небывалый доселе у нас реализм художественного изображениявремя подтвердило эту высокую оценку из статей и книг на всех языках мира посвященных анне карениной можно составить целую библиотеку я без колебаний назвал анну каренину величайшим социальным романом во всей мировой литературе  писал томас маннзначение романа толстого состоит не в эстетической ценности отдельных картин а в художественной завершенности целоговойну и мир толстой называл книгой о прошлом в начале  года он просил редактора журнала русский вестник мн каткова в оглавлении и даже в объявлении не называть его сочинение романом для меня это очень важно и потому очень прошу вас об этом   толстой мог бы обосновать свое определение жанра книга ссылкой на гегеля которого он внимательно перечитывал в годы работы над войной и миром гегель называл книгой эпические произведения связанные с целостным миром определенного народа и определенной эпохи книга или самобытная эпопея дает картину национального самосознания в нравственных устоях семейной жизни в общественных условиях состояния войны и мира курсив наш '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"анна каренина один из самых знаменитых романов льва толстого начинается ставшей афоризмом фразой все счастливые семьи похожи друг на друга каждая несчастливая семья несчастлива посвоему это книга о вечных ценностях о любви о вере о семье о человеческом достоинстве  лев толстойроман широкого дыханиячасть перваялев толстойанна каренинароман широкого дыханияанна каренина поразила современников вседневностью содержания необычайная свобода раскованность повествования удивительно сочетались в этом романе с цельностью художественного взгляда автора на жизнь он выступал здесь как художник и мыслитель и назначение искусства видел не в том чтобы неоспоримо разрешить вопрос а в том чтобы заставить любить жизнь в бесчисленных никогда не истощимых всех ее проявлениях  в е годы один маститый писатель повидимому гончаров сказал достоевскому это вещь неслыханная это вещь первая кто у нас из писателей может поравняться с этим а в европе  кто представит хоть чтонибудь подобное фм достоевский находил в новом романе толстого огромную психологическую разработку души человеческой страшную глубину и силу и главное небывалый доселе у нас реализм художественного изображениявремя подтвердило эту высокую оценку из статей и книг на всех языках мира посвященных анне карениной можно составить целую библиотеку я без колебаний назвал анну каренину величайшим социальным романом во всей мировой литературе  писал томас маннзначение романа толстого состоит не в эстетической ценности отдельных картин а в художественной завершенности целоговойну и мир толстой называл книгой о прошлом в начале  года он просил редактора журнала русский вестник мн каткова в оглавлении и даже в объявлении не называть его сочинение романом для меня это очень важно и потому очень прошу вас об этом   толстой мог бы обосновать свое определение жанра книга ссылкой на гегеля которого он внимательно перечитывал в годы работы над войной и миром гегель называл книгой эпические произведения связанные с целостным миром определенного народа и определенной эпохи книга или самобытная эпопея дает картину национального самосознания в нравственных устоях семейной жизни в общественных условиях состояния войны и мира курсив наш \"\n",
    "text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'куукяиквфупукящепуяпыяакюмцяыукюфупнмцявщюкущчяхжчкянщханщзщяукьпукфнаэяанкчбфгякйщвпыющюяйвкыщгячафяаьканхпчмфяафюжпялщцщшпяеврзяукяеврзкяикшекэяуфаьканхпчкэяафюжэяуфаьканхпчкялщачщфюря нщяиупзкящячфьумцядфуущанэцящяхотчпящячфвфящяафюжфящяьфхщчфьфаищюяещанщпуанчфяяхфчянщханщгвщюкуябпвщищзщяемцкупэьканжялфвчкэхфчянщханщгкуукяиквфупуквщюкуябпвщищзщяемцкупэкуукяиквфупукялщвкыпхкяащчвфюфуупищчячафеуфчущанжояащефвшкупэяуфщтмькгукэяачщтщекявкаищчкуущанжялщчфанчщчкупэярепчпнфхжущяащьфнкхпажячя нщюявщюкуфяаядфхжущанжояцрещшфанчфуущзщячызхэекякчнщвкяукяшпыужящуячманрлкхяыефажяикияцрещшупияпяюмахпнфхжяпяукыукьфупфяпаираанчкячпефхяуфячянщюяьнщтмяуфщалщвпющявкывфбпнжячщлвщаякячянщюяьнщтмяыканкчпнжяхотпнжяшпыужячятфаьпахфуумцяупищзекяуфяпанщъпюмцячафцяффялвщэчхфупэцяячяфязщемящепуяюканпнмгялпакнфхжялщчпепющюрязщуьквщчяаикыкхяещанщфчаищюря нщячфъжяуфахмцкуукэя нщячфъжялфвчкэяинщяряукаяпыялпакнфхфгяющшфнялщвкчуэнжаэяая нпюякячяфчвщлфяяинщялвфеанкчпняцщнжяьнщуптрежялщещтущфяйюяещанщфчаипгяукцщепхячяущчщюявщюкуфянщханщзщящзвщюуроялапцщхщзпьфаироявкывктщниряербпяьфхщчфьфаищгяанвкбуроязхртпуряпяапхряпязхкчущфяуфтмчкхмгяещафхфяряукаявфкхпыюяцрещшфанчфуущзщяпыщтвкшфупэчвфюэялщенчфвепхщя нрячмащироящдфуиряпыяанкнфгяпяиупзяукячафцяэымикцяюпвкялщачэъфуумцякууфяиквфупущгяющшущяащанкчпнжядфхроятптхпщнфиряэятфыяищхфткупгяукычкхякууряиквфупурячфхпькгбпюяащдпкхжумюявщюкущюячщячафгяюпвщчщгяхпнфвкнрвфяялпакхянщюкаяюкууыукьфупфявщюкукянщханщзщяащанщпняуфячя анфнпьфаищгядфуущанпящнефхжумцяиквнпуякячяцрещшфанчфуущгяыкчфвбфуущанпядфхщзщчщгуряпяюпвянщханщгяукымчкхяиупзщгящялвщбхщюячяукькхфяязщекящуялвщапхявфекинщвкяшрвукхкяврааипгячфанупияюуяикнищчкячящзхкчхфуппяпяекшфячящтсэчхфуппяуфяукымчкнжяфзщяащьпуфупфявщюкущюяехэяюфуэя нщящьфужячкшущяпялщнщюрящьфужялвщбрячкаящтя нщюяяянщханщгяющзятмящтщаущчкнжяачщфящлвфефхфупфяшкувкяиупзкяаамхищгяукязфзфхэяищнщвщзщящуячупюкнфхжущялфвфьпнмчкхячязщемявктщнмяукеячщгущгяпяюпвщюязфзфхжяукымчкхяиупзщгя лпьфаипфялвщпычфефупэяачэыкуумфяаядфхщанумюяюпвщюящлвфефхфуущзщяуквщекяпящлвфефхфуущгя лщцпяиупзкяпхпяакющтмнукэя лщлфэяекфняиквнпуряукдпщукхжущзщяакющащыукупэячяувкчанчфуумцяранщэцяафюфгущгяшпыупячящтъфанчфуумцярахщчпэцяащанщэупэячщгумяпяюпвкяирвапчяукбя'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text = cipher(text.lower())\n",
    "encoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Для дешифровки  воспользуемся произведением \"Война и Мир\", чтобы получить частотность токенов на русском языке**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "war_and_piece_counter_tokens = {\n",
    "    key: value for (key, value) in war_and_piece_counter.items() if key in TOKENS_LIST\n",
    "}\n",
    "war_and_piece_counter_tokens_sorted = sorted(\n",
    "    war_and_piece_counter_tokens.items(), key=lambda x: x[1], reverse=True\n",
    ")\n",
    "war_and_piece_counter_tokens_sorted = [\n",
    "    t[0] for t in war_and_piece_counter_tokens_sorted\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrypt_text(encoded_text, decrypt_keys):\n",
    "    encoded_text_counter = Counter(encoded_text)\n",
    "    encoded_text_counter_tokens = {\n",
    "        key: value\n",
    "        for (key, value) in encoded_text_counter.items()\n",
    "        if key in TOKENS_LIST\n",
    "    }\n",
    "    encoded_text_counter_sorted = sorted(\n",
    "        encoded_text_counter_tokens.items(), key=lambda x: x[1], reverse=True\n",
    "    )\n",
    "    encoded_text_counter_sorted = [t[0] for t in encoded_text_counter_sorted]\n",
    "\n",
    "    if len(decrypt_keys) != len(encoded_text_counter_sorted):\n",
    "        raise Exception(\n",
    "            f\"len decrypt_keys [{len(decrypt_keys)}] and encoded_keys [{len(encoded_text_counter_sorted)}] has different size!\"\n",
    "        )\n",
    "\n",
    "    dictionary = dict(zip(encoded_text_counter_sorted, decrypt_keys))\n",
    "    decoded_tokens = []\n",
    "\n",
    "    for i in encoded_text:\n",
    "        decoded_tokens.append(dictionary.get(i, i))\n",
    "\n",
    "    decoded_text = \"\".join(decoded_tokens)\n",
    "\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'иааи дивеанаи омна нб тикпч баикеанспч вокиаол рьли сортсояо аийнаиестг тсилюез ифовнбкок фвибоз лте тйитсрнлпе текьн ыочошн мвуя аи мвуяи дишмиг аетйитсрнлиг текьг аетйитсрнли ыотлоеку хсо даняи о лейапч цеааотсгч о рэжлн о леве о текье о йеролейетдок мотсонатсле  рел сортсозвокиа юнводояо мпчиангйитсь ыевлигрел сортсозиааи дивеанаивокиа юнводояо мпчиангиааи дивеанаи ыовибнри толвекеаандол лтемаелаотсьэ томевшианг аеожпйизаиг тложоми витдолиааотсь ыолетслолианг умнлнсерьао тойесирнть л хсок вокиае т церьаотсьэ чумошетслеааояо лбяргми илсови аи шнбаь оа лптсуыир бметь дид чумошанд н кптрнсерь н аибаийеане нтдуттсли лнмер ае л сок йсожп аеотыовнко вибвеюнсь лоывот и л сок йсожп битсилнсь рэжнсь шнбаь л жетйнтреаапч андоями ае нтсощнкпч лтеч ее ывоглреангч  л е яомп омна китснспз ынтисерь ыолнмнкоку яоайивол тдибир мотсоелтдоку хсо лещь аетрпчиааиг хсо лещь ыевлиг дсо у аит нб ынтисерез кошес ыовилагсьтг т хснк и л елвоые  дсо ывемтсилнс чось йсоанжумь ыоможаое фк мотсоелтднз аичомнр л аолок вокиае сортсояо оявокауэ ытнчороянйетдуэ вибвижосду муюн йеролейетдоз тсвиюауэ яружнау н тнру н ярилаое аежплирпз мотере у аит веирнбк чумошетслеааояо нбожвишеанглвекг ыомслевмнро хсу лптодуэ оцеаду нб тсисез н даня аи лтеч гбпдич кнви ыотлгщеаапч иаае дивеанаоз кошао тотсилнсь церуэ жнжрноседу г жеб дорежианз аиблир иаау дивеанау лернйизюнк тоцнирьапк вокиаок ло лтез кнволоз рнсевисуве  ынтир сокит киаабаийеане вокиаи сортсояо тотсонс ае л хтсеснйетдоз цеааотсн осмерьапч дивсна и л чумошетслеааоз билевюеааотсн церояолозау н кнв сортсоз аибплир даняоз о ывоюрок л аийире  яоми оа ывотнр вемидсови шуваири вуттднз летсанд ка дисдоли л оярилреанн н мише л ожъглреанн ае аибплись еяо тойнаеане вокиаок мрг кеаг хсо ойеаь лишао н ыосоку ойеаь ывоюу лит ож хсок   сортсоз коя жп ожотаолись тлое оывемереане шиави даняи ттпрдоз аи яеяерг досовояо оа ланкисерьао ыевейнсплир л яомп вижосп аим лозаоз н кнвок яеяерь аибплир даняоз хынйетдне ывонблемеанг тлгбиаапе т церотсапк кнвок оывемереааояо аивоми н оывемереааоз хыочн даняи нрн тикожпсаиг хыоыег миес дивснау аицноаирьаояо тикотобаианг л авилтслеаапч утсогч текезаоз шнбан л ожщетслеаапч утролнгч тотсоганг лозап н кнви дувтнл аию '"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_text = decrypt_text(encoded_text, war_and_piece_counter_tokens_sorted)\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3561643835616438"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(text.lower(), decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Думал будет хуже, но все равно данный метод плох, когда зашифрованное сообщение маленькое и распределение частоты токенов совсем не совпадает с распределением подсчитанным на большом произведении.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Частоты биграмм \n",
    "Вряд ли в результате получилась такая уж хорошая расшифровка, разве что если вы брали в качестве тестовых данных целые рассказы. Но и Шерлок Холмс был не так уж прост: после буквы E, которая действительно выделяется частотой, дальше он анализировал уже конкретные слова и пытался угадать, какими они могли бы быть. Я не знаю, как запрограммировать такой интуитивный анализ, так что давайте просто сделаем следующий логический шаг:\n",
    "- подсчитайте частоты биграмм (т.е. пар последовательных букв) по корпусам;\n",
    "- проведите тестирование аналогично п.1, но при помощи биграмм.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Подсчитаем n-gram  для основного произведения \"Война и Мир\" и отсортируем по частотности токенов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram = 2\n",
    "war_and_piece_text_n_gram = [\n",
    "    \"\".join(ngram)\n",
    "    for ngram in everygrams(war_and_piece_text_filtered, min_len=n_gram, max_len=n_gram)\n",
    "]\n",
    "war_and_piece_counter_tokens_sorted_n_gram = sorted(\n",
    "    Counter(war_and_piece_text_n_gram).items(), key=lambda x: x[1], reverse=True\n",
    ")\n",
    "war_and_piece_counter_tokens_sorted_n_gram = [\n",
    "    t[0] for t in war_and_piece_counter_tokens_sorted_n_gram\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrypt_text_by_n_gram(encoded_text, decrypt_keys, n_gram=2):\n",
    "    encoded_text_n_gram = [\n",
    "        \"\".join(ngram)\n",
    "        for ngram in everygrams(encoded_text, min_len=n_gram, max_len=n_gram)\n",
    "    ]\n",
    "    encoded_text_counter_sorted = sorted(\n",
    "        Counter(encoded_text_n_gram).items(), key=lambda x: x[1], reverse=True\n",
    "    )\n",
    "    encoded_text_counter_sorted = [t[0] for t in encoded_text_counter_sorted]\n",
    "\n",
    "    decrypt_keys_short = decrypt_keys[: len(encoded_text_counter_sorted)]\n",
    "\n",
    "    if len(decrypt_keys_short) != len(encoded_text_counter_sorted):\n",
    "        raise Exception(\n",
    "            f\"len decrypt_keys [{len(decrypt_keys)}] and encoded_keys [{len(encoded_text_counter_sorted)}] has different size!\"\n",
    "        )\n",
    "\n",
    "    dictionary = dict(zip(encoded_text_counter_sorted, decrypt_keys_short))\n",
    "    decoded_tokens = []\n",
    "\n",
    "    prev_exist = False\n",
    "    prev_n_gram = \"  \"\n",
    "    for i in encoded_text_n_gram:\n",
    "        if prev_exist:\n",
    "            decoded_tokens.append(prev_n_gram[1])\n",
    "            prev_exist = False\n",
    "            if i in dictionary:\n",
    "                prev_exist = True\n",
    "                prev_n_gram = dictionary.get(i)\n",
    "        else:\n",
    "            if i in dictionary:\n",
    "                decoded_tokens.append(dictionary.get(i)[0])\n",
    "                prev_exist = True\n",
    "                prev_n_gram = dictionary.get(i)\n",
    "            else:\n",
    "                decoded_tokens.append(dictionary.get(i)[0])\n",
    "                prev_exist = False\n",
    "\n",
    "    decoded_text = \"\".join(decoded_tokens)\n",
    "\n",
    "    return decoded_text + \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Попробуем расшифровать загодированное сообщение**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ил пол двси птнеиеамдоааыенвь аевсррен акаи тоитибпк зс  н  н еди доукво нгк э тусихмпскоиюреь   м ооиеч цдейзоо гафеоо уядеметвсн пметврполяйатвнитиеч цдебтво галвнитиеч цдебпооадгмгвлае  орссрпт  окш еняьвл а вкнт ияби ет  ойдот о гасот еа ттокатлркомза  аип еооии аок зс   иакаиехлыанрн  мпесискюеч иооййабтч аок зс   шил пол двси  акаиехлыанрн  мпесискуил пол двси пооиредихпоетрдгевлсжрто м  лиаз а икноенойннисквникьеченн тводгьинап рчлртбил а иооотот егтбискворее ру тм  оенаонсд ьо оае ко акаииооояь тм а икнолрзяет еовл н   оуувбаптгр ирпн плдмьжотые йа чдссрвыотьоолюшолрзяжсжшаеаыаядру тоаен еь еавсуоа луя  ебп  мо рнио ок коеш ьебникаиоихус  решдщлрио гв ааотп ок коеш ьебвыч нг риоиябгриолдмьжо озмтид я вл еннсжрннапниоа   тиуыен м шнуиоо аырж всккни оуои нпбтнеиеаач ыррх он аву тооот меусквли ыде атооллеысрмза  мамлрквлае   оагонитяпесил твае   оагооййабтвое  олн чоамдон аву  э асяеойооиргзжыияквооаеыуотп оуаравйоиое  о д н нг рйоуяиоеш ысллртооонзьь моиромза  мамлуч н еунеир он тгко акаииок зс  н  тнуаквсеноелпузтнцматлуен решрчияеулмахлееа ттокатлр  о трнысениуишгислаеолиилаеиухгз монисетбспх мза   оолн чо дбсдмпоолрзяет еовл н  амььзряевскррдгявоонфеойжеит аечл йаенуентзьвбуламдо нвуэ аеорсссн п м шнлсебленалырпооаддязвл ентилиоол двси   асяж  оеа нг риояь иензглудеяугуллвзмьдорз срисч н ехбсртилслол двсисл о дменелуооезврстм  о акаи ко г  м э алыатг  идруйрвчадоион асрк качоааилфь еавсуо акаи пк зс  н  оеа  арйнио оая уоыматлр  яьвл а ыетяно тм енол юыиетп оолрзяет еовл   выгойл вл а ыеяь тн тг нслаеалыяк зс    н еетбсрорсс   т о аищтко он еес оии наптыео аалир д аюе ирплгат схп тя луч  от усжшавеолвертбп отнухгж вссеаемаяео отьокрж вссенион еетбвиоуа  оендиивсуо акаи комавваевжвае  тнавжо бяж  аеооя квлтнавжоо аишл бчотьсае коиик зс    аснсзебтьиад тбвиоодгмотв д о  всуолниьрпорссрпо ытжр  н питат вворя иан  тые зсуаву тм  оййдкдрртбср ои нпб рчиярбн яя г н   аеалыакоитат тон еетбсрорсс   аннматлууоо аамхо овскводдсыил зооояь та у  оалыакотв д о  вл н  н  анапаетв д о  вл   ано жеорссрпаидеоаасьеяу твановйхвмадойол юыислн квеы стм н  оаасаель искв оньргм еовл еноя  ыкно геэн   лдмьсе отьшзт еовл енояятт ккноеа  ыюскв г н баеалырпоуаялеон н '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_text_n_gram = decrypt_text_by_n_gram(\n",
    "    encoded_text, war_and_piece_counter_tokens_sorted_n_gram\n",
    ")\n",
    "decoded_text_n_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07671232876712329"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(text.lower(), decoded_text_n_gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Очевидный результат т.к. кол-во токенов (биграмм) стало еще больше и если распределение для одного символа в маленьком сообщении было нерелевантным , то когда разделили на биграммы , то ситуация стала еще хуже.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 MCMC-сэмплировании\n",
    "Но и это ещё не всё: биграммы скорее всего тоже далеко не всегда работают. Основная часть задания — в том, как можно их улучшить:\n",
    "- предложите метод обучения перестановки символов в этом задании, основанный на MCMC-сэмплировании, но по-прежнему работающий на основе статистики биграмм;\n",
    "- реализуйте и протестируйте его, убедитесь, что результаты улучшились.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве вдохновения мне помогли след. источники:\n",
    "- [Applications of MCMC for Cryptography and Optimization](https://towardsdatascience.com/applications-of-mcmc-for-cryptography-and-optimization-1f99222b7132)\n",
    "- [MCMC-сэмплинг для тех, кто учился, но ничего не понял](https://habr.com/en/company/wunderfund/blog/279545/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм:\n",
    "- считаем p score для текста сисходным маппигом (p_current)\n",
    "- перестанавливаем произвольно два токена \n",
    "- пробуем рашифровать и считаем p score (p_proposal)\n",
    "- если p_proposal больше, чем p_current, полученная вероятность будет больше 1, и такое предложение мы, определённо, примем. Однако, если p_current больше чем p_proposal, скажем, в два раза, шанс перехода будет составлять уже 50%.\n",
    "\n",
    "- пробуем менять произвльно буквы n steps (steps) выбираем лучший результат \n",
    "\n",
    "Но т.к. это жадный алгоритм и можем забрести не туда, то делаем несколько итераций алгоритма\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_p_tokens(text, n_gram=2):\n",
    "    n_gram_size = len(set(text)) ** n_gram\n",
    "\n",
    "    n_grams = [\n",
    "        \"\".join(token) for token in everygrams(text, min_len=n_gram, max_len=n_gram)\n",
    "    ]\n",
    "\n",
    "    p_n_grams = {k: v / n_gram_size for k, v in Counter(n_grams).items()}\n",
    "\n",
    "    return p_n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_p_score(text, decoder, p_tokens, n_gram=2):\n",
    "    decoded_text = \"\".join([decoder.get(t, \" \") for t in text])\n",
    "\n",
    "    p_log = 0.0\n",
    "\n",
    "    for i in range(len(decoded_text) - n_gram):\n",
    "        token = decoded_text[i : i + n_gram]\n",
    "        p_log += np.log(p_tokens.get(token, 0.0001))\n",
    "\n",
    "    return p_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcmc_decoder(\n",
    "    encoded_text: str,\n",
    "    tokens_encoder: List[str],\n",
    "    tokens_decoder: List[str],\n",
    "    p_tokens,\n",
    "    iters=5,\n",
    "    steps=1500,\n",
    "    n_gram=2,\n",
    "):\n",
    "\n",
    "    p_prev = -float(\"inf\")\n",
    "    selected_encoder_decoder = dict()\n",
    "\n",
    "    for _ in range(iters):\n",
    "\n",
    "        current_token_decoder = tokens_decoder.copy()\n",
    "\n",
    "        decoder_encoder = dict()\n",
    "\n",
    "        for token_encoder, token_decoder in zip(\n",
    "            tokens_encoder, current_token_decoder[: len(tokens_encoder)]\n",
    "        ):\n",
    "            decoder_encoder[token_encoder] = token_decoder\n",
    "\n",
    "        p_current = calculate_p_score(\n",
    "            encoded_text, decoder_encoder, p_tokens, n_gram=n_gram\n",
    "        )\n",
    "\n",
    "        for _ in range(steps):\n",
    "            decoder_proposal = current_token_decoder.copy()\n",
    "\n",
    "            idx1, idx2 = np.random.choice(len(decoder_proposal), replace=False, size=2)\n",
    "\n",
    "            decoder_proposal[idx1], decoder_proposal[idx2] = (\n",
    "                decoder_proposal[idx2],\n",
    "                decoder_proposal[idx1],\n",
    "            )\n",
    "\n",
    "            decoder_encoder_proposal = dict()\n",
    "\n",
    "            for token_encoder, token_decoder in zip(\n",
    "                tokens_encoder, decoder_proposal[: len(tokens_encoder)]\n",
    "            ):\n",
    "                decoder_encoder_proposal[token_encoder] = token_decoder\n",
    "\n",
    "            p_proposal = calculate_p_score(\n",
    "                encoded_text, decoder_encoder_proposal, p_tokens, n_gram=n_gram\n",
    "            )\n",
    "\n",
    "            # если p_proposal больше, чем p_current, полученная вероятность будет больше 1, и такое предложение мы, определённо, примем.\n",
    "            # Однако, если p_current больше чем p_proposal, скажем, в два раза, шанс перехода будет составлять уже 50%:\n",
    "\n",
    "            accept = np.exp(p_proposal - p_current) > np.random.rand()\n",
    "\n",
    "            if accept:\n",
    "                current_token_decoder = decoder_proposal\n",
    "                p_current = p_proposal\n",
    "                decoder_encoder = decoder_encoder_proposal\n",
    "\n",
    "        if p_current > p_prev:\n",
    "            selected_encoder_decoder = decoder_encoder\n",
    "            p_prev = p_current\n",
    "\n",
    "    decoded_text = \"\".join([selected_encoder_decoder.get(t, \" \") for t in encoded_text])\n",
    "\n",
    "    return decoded_text, p_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_tokens = calculate_p_tokens(war_and_piece_text_filtered, n_gram=2)\n",
    "\n",
    "mcmc_decoded_text, _ = mcmc_decoder(\n",
    "    encoded_text=encoded_text,\n",
    "    tokens_encoder=TOKENS_LIST,\n",
    "    tokens_decoder=TOKENS_LIST,\n",
    "    p_tokens=p_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'анна каренина обин из садых знаденитых роданов льва толстого начинаетсм ставщей аяориздод яразой все счастливые седьи похожи бруг на бруга кажбам несчастливам седьм несчастлива посвоеду это книга о вечных фенностмх о люцви о вере о седье о человеческод бостоинстве  лев толстойродан щирокого быханимчасть первамлев толстойанна каренинародан щирокого быханиманна каренина поразила совреденников всебневностью собержаним неоцычайнам своцоба раскованность повествованим убивительно сочетались в этод родане с фельностью хубожественного взглмба автора на жизнь он выступал збесь как хубожник и дыслитель и назначение искусства вибел не в тод чтоцы неоспоридо разрещить вопрос а в тод чтоцы заставить люцить жизнь в цесчисленных никогба не истошидых всех ее промвленимх  в е гобы обин даститый писатель повибидоду гончаров сказал бостоевскоду это вешь неслыханнам это вешь первам кто у нас из писателей дожет поравнмтьсм с этид а в европе  кто пребставит хоть чтоницубь побоцное яд бостоевский нахобил в новод родане толстого огродную психологическую разрацотку бущи человеческой стращную глуцину и силу и главное нецывалый боселе у нас реализд хубожественного изоцраженимвредм побтвербило эту высокую офенку из статей и книг на всех мзыках дира посвмшенных анне карениной дожно составить фелую цицлиотеку м цез колецаний назвал анну каренину величайщид софиальныд роданод во всей дировой литературе  писал тодас даннзначение родана толстого состоит не в эстетической фенности отбельных картин а в хубожественной заверщенности фелоговойну и дир толстой называл книгой о прощлод в начале  гоба он просил ребактора журнала русский вестник дн каткова в оглавлении и баже в оцъмвлении не называть его сочинение роданод блм денм это очень важно и потоду очень прощу вас оц этод   толстой дог цы оцосновать свое опребеление жанра книга ссылкой на гегелм которого он внидательно перечитывал в гобы рацоты наб войной и дирод гегель называл книгой эпические произвебеним свмзанные с фелостныд дирод опребеленного нароба и опребеленной эпохи книга или садоцытнам эпопем бает картину нафионального садосознаним в нравственных устомх седейной жизни в оцшественных условимх состомним войны и дира курсив нащ '"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcmc_decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9114155251141552"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(text.lower(),mcmc_decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Это невероятный результат, мы смогли раcшифровать закодированное сообщение с маленькой длиной**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Но интересно попробовать распараллелить алгоритм.  Для этой задачи использовал Pool, который задается параметром n_jobs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcmc_decoder_parallelize(\n",
    "    encoded_text: str,\n",
    "    tokens_encoder: List[str],\n",
    "    tokens_decoder: List[str],\n",
    "    p_tokens,\n",
    "    iters=5,\n",
    "    steps=4000,\n",
    "    n_gram=2,\n",
    "    n_jobs=4,\n",
    "):\n",
    "    with Pool(processes=n_jobs) as pool:\n",
    "        params = (\n",
    "            encoded_text,\n",
    "            tokens_encoder,\n",
    "            tokens_decoder,\n",
    "            p_tokens,\n",
    "            1,\n",
    "            steps,\n",
    "            n_gram,\n",
    "        )\n",
    "        reponse = pool.starmap(mcmc_decoder, [params] * iters)\n",
    "        reponse.sort(key=lambda tup: tup[1], reverse=True)\n",
    "        return reponse[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_decoded_text_parallelize = mcmc_decoder_parallelize(\n",
    "    encoded_text=encoded_text,\n",
    "    tokens_encoder=TOKENS_LIST,\n",
    "    tokens_decoder=TOKENS_LIST,\n",
    "    p_tokens=p_tokens,\n",
    "    iters=10,\n",
    "    n_jobs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'анна каленина один из тамых знаменирых ломанов сьва рострого начинаертя травшей афолизмом флазой вте тчатрсивые темьи похожи длуг на длуга каждая нетчатрсивая темья нетчатрсива потвоему эро книга о вечных ценнотрях о сюбви о веле о темье о чесовечетком дотроинтрве  сев ростройломан шилокого дыханиячатрь пелваясев ростройанна калениналоман шилокого дыханияанна каленина полазиса товлеменников втедневнотрью тоделжания необычайная твобода латкованнотрь поветрвования удивиресьно точерасить в эром ломане т цесьнотрью художетрвенного взгсяда аврола на жизнь он вытрупас здеть как художник и мытсиресь и назначение иткуттрва видес не в ром чробы неотполимо лазлеширь воплот а в ром чробы затравирь сюбирь жизнь в бетчитсенных никогда не итрощимых втех ее плоявсениях  в е годы один матрирый питаресь повидимому гончалов тказас дотроевткому эро вещь нетсыханная эро вещь пелвая кро у нат из питаресей можер полавнярьтя т эрим а в евлопе  кро пледтравир хорь чронибудь подобное фм дотроевткий находис в новом ломане рострого огломную птихосогичеткую лазлаборку души чесовечеткой трлашную гсубину и тису и гсавное небывасый дотесе у нат леасизм художетрвенного изоблажениявлемя подрвелдисо эру вытокую оценку из трарей и книг на втех языках мила потвященных анне калениной можно тотравирь цесую бибсиореку я без косебаний назвас анну каленину весичайшим тоциасьным ломаном во втей миловой сиреларуле  питас ромат маннзначение ломана рострого тотроир не в этреричеткой ценнотри ордесьных калрин а в художетрвенной завелшеннотри цесоговойну и мил рострой называс книгой о плошсом в начасе  года он плотис ледакрола жулнаса лутткий ветрник мн каркова в огсавсении и даже в объявсении не называрь его точинение ломаном дся меня эро очень важно и порому очень плошу ват об эром   рострой мог бы оботноварь твое опледесение жанла книга ттыской на гегеся королого он внимаресьно пелечирывас в годы лаборы над войной и милом гегесь называс книгой эпичеткие плоизведения твязанные т цесотрным милом опледесенного налода и опледесенной эпохи книга иси тамобырная эпопея даер калрину национасьного тамотознания в нлавтрвенных утроях темейной жизни в общетрвенных утсовиях тотрояния войны и мила култив наш '"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcmc_decoded_text_parallelize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Расшифруйте сообщение:\n",
    "←⇠⇒↟↹↷⇊↹↷↟↤↟↨←↹↝⇛⇯↳⇴⇒⇈↝⇊↾↹↟⇒↟↹⇷⇛⇞↨↟↹↝⇛⇯↳⇴⇒⇈↝⇊↾↹↨←⇌⇠↨↹⇙↹⇸↨⇛↙⇛↹⇠⇛⇛↲⇆←↝↟↞↹⇌⇛↨⇛⇯⇊↾↹⇒←↙⇌⇛↹⇷⇯⇛⇞↟↨⇴↨⇈↹⇠⇌⇛⇯←←↹↷⇠←↙⇛↹↷⇊↹↷⇠←↹⇠↤←⇒⇴⇒↟↹⇷⇯⇴↷↟⇒⇈↝⇛↹↟↹⇷⇛⇒⇙⇞↟↨←↹↳⇴⇌⇠↟↳⇴⇒⇈↝⇊↾↹↲⇴⇒⇒↹⇰⇴↹⇷⇛⇠⇒←↤↝←←↹⇞←↨↷←⇯↨⇛←↹⇰⇴↤⇴↝↟←↹⇌⇙⇯⇠⇴↹↘⇛↨↞↹⇌⇛↝←⇞↝⇛↹↞↹↝↟⇞←↙⇛↹↝←↹⇛↲←⇆⇴⇏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'←⇠⇒↟↹↷⇊↹↷↟↤↟↨←↹↝⇛⇯↳⇴⇒⇈↝⇊↾↹↟⇒↟↹⇷⇛⇞↨↟↹↝⇛⇯↳⇴⇒⇈↝⇊↾↹↨←⇌⇠↨↹⇙↹⇸↨⇛↙⇛↹⇠⇛⇛↲⇆←↝↟↞↹⇌⇛↨⇛⇯⇊↾↹⇒←↙⇌⇛↹⇷⇯⇛⇞↟↨⇴↨⇈↹⇠⇌⇛⇯←←↹↷⇠←↙⇛↹↷⇊↹↷⇠←↹⇠↤←⇒⇴⇒↟↹⇷⇯⇴↷↟⇒⇈↝⇛↹↟↹⇷⇛⇒⇙⇞↟↨←↹↳⇴⇌⇠↟↳⇴⇒⇈↝⇊↾↹↲⇴⇒⇒↹⇰⇴↹⇷⇛⇠⇒←↤↝←←↹⇞←↨↷←⇯↨⇛←↹⇰⇴↤⇴↝↟←↹⇌⇙⇯⇠⇴↹↘⇛↨↞↹⇌⇛↝←⇞↝⇛↹↞↹↝↟⇞←↙⇛↹↝←↹⇛↲←⇆⇴⇏'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strange_text = '←⇠⇒↟↹↷⇊↹↷↟↤↟↨←↹↝⇛⇯↳⇴⇒⇈↝⇊↾↹↟⇒↟↹⇷⇛⇞↨↟↹↝⇛⇯↳⇴⇒⇈↝⇊↾↹↨←⇌⇠↨↹⇙↹⇸↨⇛↙⇛↹⇠⇛⇛↲⇆←↝↟↞↹⇌⇛↨⇛⇯⇊↾↹⇒←↙⇌⇛↹⇷⇯⇛⇞↟↨⇴↨⇈↹⇠⇌⇛⇯←←↹↷⇠←↙⇛↹↷⇊↹↷⇠←↹⇠↤←⇒⇴⇒↟↹⇷⇯⇴↷↟⇒⇈↝⇛↹↟↹⇷⇛⇒⇙⇞↟↨←↹↳⇴⇌⇠↟↳⇴⇒⇈↝⇊↾↹↲⇴⇒⇒↹⇰⇴↹⇷⇛⇠⇒←↤↝←←↹⇞←↨↷←⇯↨⇛←↹⇰⇴↤⇴↝↟←↹⇌⇙⇯⇠⇴↹↘⇛↨↞↹⇌⇛↝←⇞↝⇛↹↞↹↝↟⇞←↙⇛↹↝←↹⇛↲←⇆⇴⇏'\n",
    "strange_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_tokens = calculate_p_tokens(anna_kerenina_text_filtered, n_gram=2)\n",
    "\n",
    "\n",
    "mcmc_decoded_strange_text = mcmc_decoder(\n",
    "    strange_text,\n",
    "    tokens_encoder=list(set(strange_text)),\n",
    "    tokens_decoder=TOKENS_LIST,\n",
    "    p_tokens=p_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'евли сы сишите норкальный или помти норкальный тедвт у чтого воозжения доторый легдо промитать вдорее свего сы све вшелали прасильно и полумите кадвикальный залл ба повлешнее метсертое башание дурва хотя донемно я нимего не озежащ'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcmc_decoded_strange_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Дефолтные параметры mcmc алгоритма не позволили получить номальный результат. Необходимо увеличить число попыток и шагов** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_decoded_strange_text = mcmc_decoder(\n",
    "    encoded_text=strange_text,\n",
    "    tokens_encoder=list(set(strange_text)),\n",
    "    tokens_decoder=TOKENS_LIST,\n",
    "    p_tokens=p_tokens,\n",
    "    iters=10,\n",
    "    steps=8000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'если вы вимите норкальный или почти норкальный тедст у этого сообщения доторый легдо прочитать сдорее всего вы все смелали правильно и получите кадсикальный балл за послемнее четвертое замание дурса хотя донечно я ничего не обещаъ'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcmc_decoded_strange_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Уже лучше, но попробуем еще немного увеличить кол-во шагов и попыток**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_decoded_strange_text = mcmc_decoder(\n",
    "    encoded_text=strange_text,\n",
    "    tokens_encoder=list(set(strange_text)),\n",
    "    tokens_decoder=TOKENS_LIST,\n",
    "    p_tokens=p_tokens,\n",
    "    iters=20,\n",
    "    steps=20000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_decoded_strange_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Отлично ! получилось расшифровать загадочный текст**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alfa",
   "language": "python",
   "name": "alfa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
